{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Evolutionary Strategies and Reinforcement Learning applied to Minesweeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**\n",
    "\n",
    "_Jacob J. Hansen (s134097), Jakob D. Havtorn (s132315),_\n",
    "\n",
    "_Mathias G. Johnsen (s123249) and Andreas T. Kristensen (s144026)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import pstats\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.models import Input, Model, Sequential, clone_model, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from minesweeper_tk import Minesweeper\n",
    "\n",
    "save_dir = ''\n",
    "mp.freeze_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolutionary Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define fitness evaluation function\n",
    "def fitnessfun(env, model):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    steps = 0\n",
    "    while not done and steps < rows*cols-mines:\n",
    "        # Predict action\n",
    "        action = model.predict(observation.reshape((1, rows, cols, n_chs)))\n",
    "        # Mask invalid moves (no need to renormalize when argmaxing)\n",
    "        mask = env.get_validMoves().flatten()\n",
    "        action[0, ~mask] = 0\n",
    "        # Step and get reward\n",
    "        observation, reward, done, info = env.step(np.argmax(action))\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    win = True if done and reward is 0.9 else False\n",
    "    return total_reward, {'steps': steps, 'win': win}\n",
    "\n",
    "# Define test function\n",
    "def testfun(env, model, episodes):\n",
    "    total_reward = 0\n",
    "    wins = 0\n",
    "    for i in range(episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done and t < rows*cols-mines:\n",
    "            action = model.predict(observation.reshape((1, rows, cols, n_chs)))\n",
    "            observation, reward, done, info = env.step(np.argmax(action))\n",
    "            total_reward += reward\n",
    "            t += 1\n",
    "        if i % 100 == 0:\n",
    "            print('Episode: {: >3d}'.format(i))\n",
    "        wins += 1 if done and reward is 0.9 else 0\n",
    "    return total_reward/episodes, t, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define environment \n",
    "rows = 4\n",
    "cols = 4\n",
    "mines = 4\n",
    "OUT = 'FULL'\n",
    "rewards_structure = {\"win\": 0.9, \"loss\": -1, \"progress\": 0.9, \"noprogress\": -0.3, \"YOLO\": -0.3}\n",
    "env = Minesweeper(display=False, OUT=OUT, ROWS=rows, COLS=cols, MINES=mines, rewards=rewards_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 15)          3765      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 35)          4760      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 560)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               112200    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                3216      \n",
      "=================================================================\n",
      "Total params: 244,541\n",
      "Trainable params: 244,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "obs = env.reset()\n",
    "n_chs = obs.shape[-1]\n",
    "n_hidden = [200, 200, 200, 200]\n",
    "n_outputs = rows*cols\n",
    "model = Sequential()\n",
    "# Convs\n",
    "model.add(Conv2D(15, (5, 5), input_shape=(rows, cols, n_chs), padding='same', activation='relu'))\n",
    "model.add(Conv2D(35, (3, 3), padding='same', activation='relu'))\n",
    "# Dense\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=n_hidden[0],\n",
    "                activation='relu',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,#l2(reg),\n",
    "                bias_regularizer=None))#l2(reg)))\n",
    "# Hidden\n",
    "for n_units in n_hidden[1:]:\n",
    "    model.add(Dense(units=n_units,\n",
    "                    activation='relu',\n",
    "                    kernel_initializer='glorot_uniform',\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=None,#l2(reg),\n",
    "                    bias_regularizer=None))#l2(reg)))\n",
    "# Output\n",
    "model.add(Dense(units=n_outputs,\n",
    "                activation='softmax',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Train)\n",
    "do_train = False\n",
    "if do_train:\n",
    "    from context import core\n",
    "    from core.strategies import ES\n",
    "    regu = 0.01\n",
    "    nags = 20\n",
    "    lrte = 0.01\n",
    "    sigm = 0.01\n",
    "    cint = 100\n",
    "    nwrk = mp.cpu_count()\n",
    "    e = ES(fun=fitnessfun, model=model, env=env, reg={'L2': regu}, population=nags, learning_rate=lrte, sigma=sigm, workers=nwrk, save_dir=save_dir)\n",
    "    e.evolve(ngns, checkpoint_every=cint, plot_every=cint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:   0\n",
      "Episode: 100\n",
      "Episode: 200\n",
      "Episode: 300\n",
      "Episode: 400\n",
      "Episode: 500\n",
      "Episode: 600\n",
      "Episode: 700\n",
      "Episode: 800\n",
      "Episode: 900\n",
      "Win rate: 0.0\n",
      "Average test reward: -0.38010000000001887\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "test_episodes = 1000\n",
    "model = load_model('es-model.h5')\n",
    "env = Minesweeper(display=False, OUT=OUT, ROWS=rows, COLS=cols, MINES=mines, rewards=rewards_structure)\n",
    "\n",
    "# Run game env and save rewards and winrate for 100 games\n",
    "average_reward, _, wins = testfun(env, model, test_episodes)\n",
    "print('Win rate: {}'.format(wins/test_episodes))\n",
    "print('Average test reward: {}'.format(average_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the function\n",
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "sys.path.append('q_learning')\n",
    "from train import *\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test minesweeper model on 6x6 board with 6 mines\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 90.20\n"
     ]
    }
   ],
   "source": [
    "# Get the 90.20 win-rate\n",
    "setup_model(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test minesweeper model on 6x6 board with a random number of mines\n",
      "\n",
      "Testing with best model on random number of mines\n",
      "Mines = 1\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 75.48\n",
      "Mines = 2\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 100.00\n",
      "Mines = 3\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 100.00\n",
      "Mines = 4\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 99.03\n",
      "Mines = 5\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 96.05\n",
      "Mines = 6\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 88.45\n",
      "Mines = 7\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 75.64\n",
      "Mines = 8\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 60.37\n",
      "Mines = 9\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 43.80\n",
      "Mines = 10\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 28.28\n",
      "Mines = 11\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 18.05\n",
      "Mines = 12\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best-random\n",
      "  Win Rate: 11.70\n",
      "\n",
      "Testing with best model on board with 6 mines\n",
      "Mines = 1\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 0.91\n",
      "Mines = 2\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 81.38\n",
      "Mines = 3\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 60.75\n",
      "Mines = 4\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 68.85\n",
      "Mines = 5\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 87.29\n",
      "Mines = 6\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 93.25\n",
      "Mines = 7\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 90.39\n",
      "Mines = 8\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 76.96\n",
      "Mines = 9\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 52.11\n",
      "Mines = 10\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 24.89\n",
      "Mines = 11\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 9.44\n",
      "Mines = 12\n",
      "INFO:tensorflow:Restoring parameters from /home/andreas/github/minesweeper_solver/results-ipynb/q_learning/output_best/checkpoints_minesweeper/model-best\n",
      "  Win Rate: 3.21\n"
     ]
    }
   ],
   "source": [
    "# Get the win-rates for different number of mines\n",
    "tf.reset_default_graph()\n",
    "setup_model(\"test_random_mines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network which obtained 90.20% win-rate on 6x6 board with 6 mines\n",
      "Initializing replay memory 0/50000\n",
      "Initializing replay memory 1000/50000\n",
      "Initializing replay memory 2000/50000\n",
      "Initializing replay memory 3000/50000\n",
      "Initializing replay memory 4000/50000\n",
      "Initializing replay memory 5000/50000\n",
      "Initializing replay memory 6000/50000\n",
      "Initializing replay memory 7000/50000\n",
      "Initializing replay memory 8000/50000\n",
      "Initializing replay memory 9000/50000\n",
      "Initializing replay memory 10000/50000\n",
      "Initializing replay memory 11000/50000\n",
      "Initializing replay memory 12000/50000\n",
      "Initializing replay memory 13000/50000\n",
      "Initializing replay memory 14000/50000\n",
      "Initializing replay memory 15000/50000\n",
      "Initializing replay memory 16000/50000\n",
      "Initializing replay memory 17000/50000\n",
      "Initializing replay memory 18000/50000\n",
      "Initializing replay memory 19000/50000\n",
      "Initializing replay memory 20000/50000\n",
      "Initializing replay memory 21000/50000\n",
      "Initializing replay memory 22000/50000\n",
      "Initializing replay memory 23000/50000\n",
      "Initializing replay memory 24000/50000\n",
      "Initializing replay memory 25000/50000\n",
      "Initializing replay memory 26000/50000\n",
      "Initializing replay memory 27000/50000\n",
      "Initializing replay memory 28000/50000\n",
      "Initializing replay memory 29000/50000\n",
      "Initializing replay memory 30000/50000\n",
      "Initializing replay memory 31000/50000\n",
      "Initializing replay memory 32000/50000\n",
      "Initializing replay memory 33000/50000\n",
      "Initializing replay memory 34000/50000\n",
      "Initializing replay memory 35000/50000\n",
      "Initializing replay memory 36000/50000\n",
      "Initializing replay memory 37000/50000\n",
      "Initializing replay memory 38000/50000\n",
      "Initializing replay memory 39000/50000\n",
      "Initializing replay memory 40000/50000\n",
      "Initializing replay memory 41000/50000\n",
      "Initializing replay memory 42000/50000\n",
      "[2017-12-28 20:20] It. 000500, Replay = 58960, epsilon = 0.9980, Episodes = 8460, Steps = 50501, Avg.R = -1.299, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0000, Avg.Loss = 0.270668, lr = 0.000250\n",
      "[2017-12-28 20:20] It. 001000, Replay = 59545, epsilon = 0.9960, Episodes = 8545, Steps = 51001, Avg.R = -1.302, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0000, Avg.Loss = 0.191056, lr = 0.000250\n",
      "[2017-12-28 20:21] It. 001500, Replay = 60126, epsilon = 0.9940, Episodes = 8626, Steps = 51501, Avg.R = -1.304, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0089, Avg.Loss = 0.146491, lr = 0.000250\n",
      "[2017-12-28 20:22] It. 002000, Replay = 60730, epsilon = 0.9920, Episodes = 8730, Steps = 52001, Avg.R = -1.299, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0166, Avg.Loss = 0.116022, lr = 0.000250\n",
      "[2017-12-28 20:23] It. 002500, Replay = 61319, epsilon = 0.9900, Episodes = 8819, Steps = 52501, Avg.R = -1.297, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0201, Avg.Loss = 0.094571, lr = 0.000250\n",
      "[2017-12-28 20:24] It. 003000, Replay = 61893, epsilon = 0.9880, Episodes = 8893, Steps = 53001, Avg.R = -1.295, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0266, Avg.Loss = 0.075980, lr = 0.000250\n",
      "[2017-12-28 20:24] It. 003500, Replay = 62466, epsilon = 0.9860, Episodes = 8966, Steps = 53501, Avg.R = -1.295, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0421, Avg.Loss = 0.061870, lr = 0.000250\n",
      "[2017-12-28 20:25] It. 004000, Replay = 63051, epsilon = 0.9840, Episodes = 9051, Steps = 54001, Avg.R = -1.293, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0573, Avg.Loss = 0.050821, lr = 0.000250\n",
      "[2017-12-28 20:26] It. 004500, Replay = 63630, epsilon = 0.9820, Episodes = 9130, Steps = 54501, Avg.R = -1.291, Max.R = 5.600, Win = 0.0, Avg.Q = 0.0754, Avg.Loss = 0.041385, lr = 0.000250\n",
      "[2017-12-28 20:27] It. 005000, Replay = 64221, epsilon = 0.9800, Episodes = 9221, Steps = 55001, Avg.R = -1.290, Max.R = 5.600, Win = 0.0, Avg.Q = 0.1022, Avg.Loss = 0.034914, lr = 0.000250\n",
      "[2017-12-28 20:28] It. 005500, Replay = 64799, epsilon = 0.9780, Episodes = 9299, Steps = 55501, Avg.R = -1.291, Max.R = 5.600, Win = 0.0, Avg.Q = 0.1179, Avg.Loss = 0.029466, lr = 0.000250\n",
      "[2017-12-28 20:28] It. 006000, Replay = 65383, epsilon = 0.9760, Episodes = 9383, Steps = 56001, Avg.R = -1.292, Max.R = 5.600, Win = 0.0, Avg.Q = 0.1419, Avg.Loss = 0.024060, lr = 0.000250\n",
      "[2017-12-28 20:29] It. 006500, Replay = 65967, epsilon = 0.9740, Episodes = 9467, Steps = 56501, Avg.R = -1.291, Max.R = 5.600, Win = 0.0, Avg.Q = 0.1634, Avg.Loss = 0.020936, lr = 0.000250\n",
      "[2017-12-28 20:30] It. 007000, Replay = 66548, epsilon = 0.9720, Episodes = 9548, Steps = 57001, Avg.R = -1.291, Max.R = 5.600, Win = 0.0, Avg.Q = 0.1871, Avg.Loss = 0.018223, lr = 0.000250\n",
      "[2017-12-28 20:31] It. 007500, Replay = 67137, epsilon = 0.9700, Episodes = 9637, Steps = 57501, Avg.R = -1.289, Max.R = 5.600, Win = 0.0, Avg.Q = 0.2155, Avg.Loss = 0.015496, lr = 0.000250\n",
      "[2017-12-28 20:32] It. 008000, Replay = 67724, epsilon = 0.9680, Episodes = 9724, Steps = 58001, Avg.R = -1.289, Max.R = 5.600, Win = 0.0, Avg.Q = 0.2467, Avg.Loss = 0.013625, lr = 0.000250\n",
      "[2017-12-28 20:32] It. 008500, Replay = 68291, epsilon = 0.9660, Episodes = 9791, Steps = 58501, Avg.R = -1.290, Max.R = 5.600, Win = 0.0, Avg.Q = 0.2812, Avg.Loss = 0.012309, lr = 0.000250\n",
      "[2017-12-28 20:33] It. 009000, Replay = 68861, epsilon = 0.9640, Episodes = 9861, Steps = 59001, Avg.R = -1.289, Max.R = 5.600, Win = 0.0, Avg.Q = 0.3173, Avg.Loss = 0.010743, lr = 0.000250\n",
      "[2017-12-28 20:34] It. 009500, Replay = 69446, epsilon = 0.9620, Episodes = 9946, Steps = 59501, Avg.R = -1.289, Max.R = 5.600, Win = 0.0, Avg.Q = 0.3519, Avg.Loss = 0.010123, lr = 0.000250\n",
      "[2017-12-28 20:35] It. 010000, Replay = 70031, epsilon = 0.9600, Episodes = 10031, Steps = 60001, Avg.R = -1.286, Max.R = 6.500, Win = 0.0, Avg.Q = 0.3958, Avg.Loss = 0.009610, lr = 0.000250\n",
      "[2017-12-28 20:36] It. 010500, Replay = 70611, epsilon = 0.9580, Episodes = 10111, Steps = 60501, Avg.R = -1.285, Max.R = 6.500, Win = 0.0, Avg.Q = 0.4335, Avg.Loss = 0.008703, lr = 0.000250\n",
      "[2017-12-28 20:37] It. 011000, Replay = 71193, epsilon = 0.9560, Episodes = 10193, Steps = 61001, Avg.R = -1.285, Max.R = 6.500, Win = 0.0, Avg.Q = 0.4708, Avg.Loss = 0.008274, lr = 0.000250\n",
      "[2017-12-28 20:37] It. 011500, Replay = 71756, epsilon = 0.9540, Episodes = 10256, Steps = 61501, Avg.R = -1.282, Max.R = 6.500, Win = 0.0, Avg.Q = 0.5229, Avg.Loss = 0.008059, lr = 0.000250\n",
      "[2017-12-28 20:38] It. 012000, Replay = 72345, epsilon = 0.9520, Episodes = 10345, Steps = 62001, Avg.R = -1.281, Max.R = 6.500, Win = 0.0, Avg.Q = 0.5630, Avg.Loss = 0.007571, lr = 0.000250\n",
      "[2017-12-28 20:39] It. 012500, Replay = 72914, epsilon = 0.9500, Episodes = 10414, Steps = 62501, Avg.R = -1.279, Max.R = 6.500, Win = 0.0, Avg.Q = 0.6077, Avg.Loss = 0.007485, lr = 0.000250\n",
      "[2017-12-28 20:40] It. 013000, Replay = 73497, epsilon = 0.9480, Episodes = 10497, Steps = 63001, Avg.R = -1.277, Max.R = 6.500, Win = 0.0, Avg.Q = 0.6583, Avg.Loss = 0.007160, lr = 0.000250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c435fbb74cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/minesweeper_solver/results-ipynb/q_learning/train.py\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5e5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train_random_mines\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/minesweeper_solver/results-ipynb/q_learning/train.py\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mqagent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mqagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mqagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_mine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/minesweeper_solver/results-ipynb/q_learning/agent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m                                                                 \u001b[0;31m# self.train_iteration is the true train iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sel_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training Q-network.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/minesweeper_solver/results-ipynb/q_learning/agent.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, sess, iteration, saver)\u001b[0m\n\u001b[1;32m    240\u001b[0m             qvalues_train = sess.run(\n\u001b[1;32m    241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpl_screens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscreens_1\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             )\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "setup_model(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d8ec3f6ced4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mwinrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#os._exit(00) # Restart jyputer Python kernel\n",
    "# Load model weights\n",
    "# Run game env and save rewards and winrate for 100 games\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "epochs = []\n",
    "steps = []\n",
    "train_r = []\n",
    "loss = []\n",
    "winrate = []\n",
    "\n",
    "stats = pickle.load(open(\"minesweeper/stats.p\", \"rb\"))\n",
    "for stat in stats:\n",
    "    steps.append(stat[1])\n",
    "    train_r.append(stat[2]/1.5)\n",
    "    loss.append(stat[3])\n",
    "    winrate.append(stat[4])\n",
    "\n",
    "   \n",
    "plt.rcParams.update({'font.size': 33}) \n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax1.plot(steps, train_r, steps, loss)\n",
    "ax1.grid()\n",
    "ax1.legend([\"train reward\", \"loss\"])\n",
    "\n",
    "ax2.plot(steps, winrate, linewidth = 4.0)\n",
    "ax2.legend([\"win rate\"])\n",
    "plt.xlabel('Steps / Actions')\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot rewards and winrate in a single plot for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
